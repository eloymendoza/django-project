<!DOCTYPE html>
<html>
<head>
    <title>Detección de Rostros en Tiempo Real</title>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
        
    <style>
        body {
            margin: 0;
            position: relative;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        canvas {
            position: absolute;
            top: -5px; /* Ajusta este valor para subir el cuadro de detección */
            left: 0;
        }


        video {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>
<body>
    <video id="video" width="720" height="560" autoplay muted></video>
    <canvas id="canvas"></canvas>
    <script>
        let canvas, displaySize;
        let isFrameSent = false; // Variable de control para evitar envíos múltiples
        const video = document.getElementById('video');
        const captureButton = document.getElementById('capture');

            Promise.all([
            faceapi.nets.faceExpressionNet.loadFromUri('/static/models') ,
                faceapi.nets.tinyFaceDetector.loadFromUri('/static/models'),
                faceapi.nets.faceLandmark68Net.loadFromUri('/static/models'),
                // Carga aquí otros modelos si los necesitas
            ]).then(startVideo);

            function startVideo() {
                navigator.getUserMedia(
                  { video: {} },
                  stream => {
                    video.srcObject = stream;
                    // Esperar a que el video cargue sus metadatos para obtener sus dimensiones
                    video.onloadedmetadata = () => {
                      video.width = video.videoWidth;
                      video.height = video.videoHeight;
                      detectFaces(); // Iniciar detección de rostros después de establecer las dimensiones
                    };
                  },
                  err => console.error(err)
                )
              }
              

        video.addEventListener('loadedmetadata', () => {
            // Crear canvas y añadirlo al DOM
            canvas = faceapi.createCanvasFromMedia(video);
            document.body.append(canvas);
            displaySize = { width: video.videoWidth, height: video.videoHeight };
            faceapi.matchDimensions(canvas, displaySize);
            detectFaces(); // Ya no necesitas pasar canvas y displaySize
        });

        async function detectFaces() {
            const options = new faceapi.TinyFaceDetectorOptions();
            let detections = await faceapi.detectAllFaces(video, options)
              .withFaceLandmarks()
              .withFaceExpressions();
          
            const displaySize = { width: video.videoWidth, height: video.videoHeight };
            faceapi.matchDimensions(canvas, displaySize);
          
            const resizedDetections = faceapi.resizeResults(detections, displaySize);
            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
            faceapi.draw.drawDetections(canvas, resizedDetections);
            faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
            faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
          
            function calculateEyeOpenness(eyePoints) {
                const verticalDistance = faceapi.euclideanDistance([eyePoints[1].x, eyePoints[1].y], [eyePoints[5].x, eyePoints[5].y]);
                const horizontalDistance = faceapi.euclideanDistance([eyePoints[0].x, eyePoints[0].y], [eyePoints[3].x, eyePoints[3].y]);
                return verticalDistance / horizontalDistance; // Proporción de apertura del ojo
              }
              
            let lastBlinkTimestamp = 0;
            const blinkThreshold = 0.3; // Ajusta este valor
            const blinkCooldown = 1000; // Tiempo mínimo entre parpadeos, en milisegundos


            let eyeOpennessHistory = []; // Para almacenar la historia de la apertura de los ojos
            const historyLength = 10; // Cuántos frames para calcular el promedio móvil

            // Modifica la función calculateEyeOpenness para actualizar la historia de apertura de ojos
            function calculateEyeOpenness(eyePoints) {
                const verticalDistance = faceapi.euclideanDistance([eyePoints[1].x, eyePoints[1].y], [eyePoints[5].x, eyePoints[5].y]);
                const horizontalDistance = faceapi.euclideanDistance([eyePoints[0].x, eyePoints[0].y], [eyePoints[3].x, eyePoints[3].y]);
                const openness = verticalDistance / horizontalDistance;

                // Actualiza la historia de apertura de ojos
                eyeOpennessHistory.push(openness);
                if (eyeOpennessHistory.length > historyLength) {
                    eyeOpennessHistory.shift(); // Mantén el tamaño de la historia
                }

                return openness;
            }

            
            resizedDetections.forEach(detection => {
              const now = Date.now();
              const landmarks = detection.landmarks.positions;
            
              // Índices de los puntos de referencia para los ojos izquierdo y derecho
              const leftEyeIndices = [36, 37, 38, 39, 40, 41];
              const rightEyeIndices = [42, 43, 44, 45, 46, 47];
            
              const leftEyePoints = leftEyeIndices.map(index => landmarks[index]);
              const rightEyePoints = rightEyeIndices.map(index => landmarks[index]);
            
              const leftEyeOpenness = calculateEyeOpenness(leftEyePoints);
              const rightEyeOpenness = calculateEyeOpenness(rightEyePoints);
            
              // Verifica si la detección de un parpadeo es válida
              if (leftEyeOpenness < blinkThreshold && rightEyeOpenness < blinkThreshold) {
                if (now - lastBlinkTimestamp > blinkCooldown) {
                  console.log("Blink detected!");
                  lastBlinkTimestamp = now;
                }
              }
            });
            
            requestAnimationFrame(detectFaces);
              
         
        
        }
          
          // Asegúrate de llamar a detectFaces inicialmente o después de cargar el video
          detectFaces();
          

        function sendFrame() {

            if (isFrameSent) {
                return; // Evitar que la función se ejecute si ya se ha enviado el frame
            }

            isFrameSent = true; // Actualizar la variable de control

            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            const data = canvas.toDataURL('image/jpeg');
            fetch('../api/ReconFacial/', {
                method: 'POST',
                body: JSON.stringify({ imagen_base64: data }),
                headers: {
                    'Content-Type': 'application/json',
                    'X-CSRFToken': getCookie('csrftoken')
                }
            })
                .then(response => response.json())
                .then(data => {
                    console.log(data);
                    if (data.match) {
                        // Si hay coincidencia, ocultamos el video y el canvas
                        video.style.display = 'none';
                        canvas.style.display = 'none';
                        // Detener la detección y el stream de la cámara
                        stopCameraStream();
                    } else {
                        alert("usuario no encontrado");
                        // Opcional: resetear isFrameSent si quieres permitir reintentos
                        isFrameSent = false;
                    }
                })
                .catch(error => {
                    console.error('Error al enviar la imagen:', error);
                    // Opcional: resetear isFrameSent si quieres permitir reintentos
                    isFrameSent = false;
                });

        }

        function stopCameraStream() {
            let stream = video.srcObject;
            if (stream) {
                let tracks = stream.getTracks();
                tracks.forEach(track => track.stop());
            }
            video.srcObject = null;
        }


        // Función para obtener el CSRF token de Django
        function getCookie(name) {
            let cookieValue = null;
            if (document.cookie && document.cookie !== '') {
                const cookies = document.cookie.split(';');
                for (let i = 0; i < cookies.length; i++) {
                    const cookie = cookies[i].trim();
                    if (cookie.substring(0, name.length + 1) === (name + '=')) {
                        cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                        break;
                    }
                }
            }
            return cookieValue;
        }
    </script>
</body>
</html>